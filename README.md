# Word_Segmentation_Tools
分词工具整理

一、	Nlpir
官方下载地址：
			https://github.com/NLPIR-team/NLPIR
			我已经下好、打包放进文件夹里了。
			NLPIR工具每七天要更新一次License, 去官网下载下面的文件覆盖源文件里的License文件。
 

二、	Pyltp
pyltp 是 LTP 的 Python 封装，提供了分词，词性标注，命名实体识别，依存句法分析，语义角色标注的功能。（目前pyltp 暂不提供语义依存分析功能。）。

Pyltp文档：
https://pyltp.readthedocs.io/zh_CN/latest/

注意：pyltp不支持python7及以上版本；要先下载好VC2015。
 
下载模型：
http://ltp.ai/download.html

我放了代码，可直接用。
 

三、	Jieba
pip安装：
pip install jieba -i https://pypi.douban.com/simple/
（使用清华镜像，超快）


四、	Hanlp
官方下载地址：
		https://github.com/hankcs/HanLP/tree/1.x
第二步：我已经附上项目，在eclipse里导入项目。点开下图文件夹，即可使用Hanlp各个功能。
 

五、	Snownlp
pip安装：
pip install snownlp

我已经附上代码
 
